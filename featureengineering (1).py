# -*- coding: utf-8 -*-
"""featureengineering.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/10Egjcr0hC7g0ouXrn79wOn0GmMVG9iXc
"""

#Feature engineering is the art of transforming raw data into meaningful features that can improve the performance of machine learning models
#Feature engineering can:
#Improve Model Performance: Well-engineered features can help your models generalize better, leading to higher accuracy.
#Reduce Overfitting: Carefully crafted features can make your models more robust and less prone to overfitting.
#Enhance Interpretability: Creating meaningful features can provide insights into how your model is making predictions.
import os
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np
from sklearn.preprocessing import LabelEncoder
from sklearn.feature_extraction import DictVectorizer
from sklearn.preprocessing import StandardScaler

df = pd.read_csv('column_2C_weka.csv')
missingvalues = df.isnull().sum()
print(missingvalues)

# 7 ways to handle missing values in the dataset
# Deleting Rows with missing values
# Impute missing values for continuous variable
# Impute missing values for categorical variable
# Other Imputation Methods
# Using Algorithms that support missing values
# Prediction of missing values
# Imputation using Deep Learning Library — Datawig

!pip install missingno
import missingno as msno
msno.matrix(df)
#Visualization of Missing Values: white lines denote the presence of missing value

#Delete Rows with Missing Values:
df.dropna(inplace = True)
print(df.isnull().sum())
print(df.shape)
# Pros:
# A model trained with the removal of all missing values creates a robust model.

# Cons:
#Loss of a lot of information.
#Works poorly if the percentage of missing values is excessive in comparison to the complete dataset.

#Impute missing values with Mean/Median:

#You can fill missing values with a specific number, the mean, median, or even create a binary column indicating whether a value was missing.
# Fill missing values with the mean
df['pelvic_incidence'] = df['pelvic_incidence'].fillna(df['pelvic_incidence'].mean())

# Pros:
#Prevent data loss which results in deletion of rows or columns
#Works well with a small dataset and is easy to implement.

# Cons:
#Works only with numerical continuous variables.
#Can cause data leakage
#Do not factor the covariance between features.

# Imputation method for categorical columns:
# When missing values is from categorical columns (string or numerical) then the missing values can be replaced with the most frequent category.
df['class'] = df['class'].fillna('U')

# Pros:
#Prevent data loss which results in deletion of rows or columns
#Works well with a small dataset and is easy to implement.
#Negates the loss of data by adding a unique category

# Cons:
#Works only with categorical variables.
#Addition of new features to the model while encoding, which may result in poor performance

#For example, for the data variable having longitudinal behavior, it might make sense to use the last valid observation to fill the missing value. This is known as the Last observation carried forward (LOCF) method.
#For the time-series dataset variable, it makes sense to use the interpolation of the variable before and after a timestamp for a missing value.

#Using Algorithms that support missing values:
#The k-NN algorithm can ignore a column from a distance measure when a value is missing. Naive Bayes can also support missing values when making a prediction. These algorithms can be used when the dataset contains null or missing values.
#Another algorithm that can be used here is RandomForest that works well on non-linear and categorical data. It adapts to the data structure taking into consideration the high variance or the bias, producing better results on large datasets.

#Pros:
#No need to handle missing values in each column as ML algorithms will handle them efficiently.

#Cons:
#No implementation of these ML algorithms in the scikit-learn library.

## Create a binary column for missing values
df['has_missing_pelvic_incidence'] = df['pelvic_incidence'].isnull().astype(int)
print(df['has_missing_pelvic_incidence'])

# One-hot encoding

#One-hot encoding is a common preprocessing technique used when working with categorical data in machine learning. It serves two key purposes:
#A) Converting categorical values into numeric vectors that algorithms like neural networks and regression can understand. Many models require numeric input.
#B) Representing categorical values in a way that captures their uniqueness. Without one-hot encoding, algorithms may incorrectly treat different categories as the same value.

# Veri kümesindeki kategorik değişkenler one-hot encoding ile sayısal formata dönüştürülür ve bu dönüştürülmüş veri kullanılarak makine öğrenimi modelleri eğitilebilir.
df_encoded = pd.get_dummies(df, columns=['class'], prefix = 'class',drop_first=True)
df_encoded['class_Normal'] = df_encoded['class_Normal'].astype(int)
print(df_encoded)

# Label encoding
#Çoğu makine öğrenimi algoritması sayısal girdilerle daha iyi çalışır. Ancak, dikkat edilmesi gereken önemli bir nokta, sayısal dönüşümün gerçekten gerekliliğidir.
# Label encoding is suitable for categorical data where there is an inherent order or ranking among the categories.
# In cases where the categorical variables are nominal or lack a meaningful order, techniques like one-hot encoding or ordinal encoding may be more appropriate.
label_encoder = LabelEncoder()
df['class_encoded'] = label_encoder.fit_transform(df['class'])
print(df[['class', 'class_encoded']])

#Özellik ölçeklendirme, makine öğreniminde bağımsız değişken veya özellik aralığının normalleştirildiği çok önemli bir ön işleme adımıdır. Birçok makine öğrenimi algoritmasında, daha geniş aralıklara sahip özellikler amaç fonksiyonuna hakim olabilir ve bu da performansın düşmesine neden olabilir. Özellik ölçeklendirme, her özelliğin sonuca eşit katkıda bulunmasını sağlayarak bu durumu hafifletmeye yardımcı olur.

#Özellik ölçeklendirmenin birkaç yaygın yöntemi vardır:

#Min-Maks Ölçeklendirme (Normalleştirme) : Bu teknik, verileri genellikle 0'dan 1'e kadar sabit bir aralığa ölçeklendirir.
#Standardizasyon (Z-skoru Normalizasyonu) : Bu yöntem, verileri ortalaması 0 ve standart sapması 1 olacak şekilde ölçeklendirir.
#Sağlam Ölçeklendirme : Bu teknik standardizasyona benzer ancak ölçeklendirme için medyan ve çeyrekler arası aralığı (IQR) kullanır. Aykırı değerlere karşı daha az duyarlıdır.
#MaxAbs Ölçeklendirme : Bu, her özelliği maksimum mutlak değerine göre ölçeklendirir ve sonuçta [-1, 1] aralığı elde edilir.

#Kategorik verileri ölçeklendirmek yerine, öncelikle onları sayısal değerlere dönüştürmek daha mantıklı olacaktır.
#Eğer sütun bir hedef değişkeni değilse, yani tahminlemek istediğiniz bir şey değilse, çıkarılması genellikle en iyi seçenektir.
#Ancak, eğer bu sütun bir hedef değişkeniyse, yani tahminlemek istediğiniz bir şeyse, o zaman onu dönüştürmeniz gerekir.
df['class'] = df['class'].apply(lambda x: 0 if x == 'Abnormal' else 1)
df

from sklearn.preprocessing import MinMaxScaler

# Min-Max Scaling
scaler = MinMaxScaler()
scaled_data = scaler.fit_transform(df)

scaled_df = pd.DataFrame(scaled_data, columns=df.columns)

print("Min-Max Scaled Data:")
print(scaled_df)

# 2. Standardization (Z-score Normalization)
# Standardization (Z-score Normalization): This method scales the data so that it has a mean of 0 and a standard deviation of 1.
scaler = StandardScaler() # Bu işlem, veri setindeki gelir bilgisini belirli bir ortalama ve standart sapma kullanarak standart normal dağılıma dönüştürmeye yarar.
df['incidence_scaled'] = scaler.fit_transform(df[['pelvic_incidence']]) #Önce fit yöntemi ile veri kümesinin istatistiklerini (ortalama ve standart sapma) hesaplar, sonra bu istatistikleri kullanarak veriyi dönüştürür.
print(df) #. Bu değerler, orijinal gelir değerlerinin ortalama ve standart sapmaya göre ne kadar uzakta olduğunu gösterir ve genellikle ortalama 0 ve standart sapma 1 olacak şekilde dönüştürülür.

#3. Robust Scaling
#This technique is similar to standardization but uses the median and the interquartile range (IQR) for scaling. It is less sensitive to outliers.
from sklearn.preprocessing import RobustScaler

df = pd.DataFrame(df)

scaler = RobustScaler()
scaled_data = scaler.fit_transform(df)

scaled_df = pd.DataFrame(scaled_data, columns=df.columns)

print(scaled_df)

#4. MaxAbs Scaling
#This scales each feature by its maximum absolute value, resulting in a range of [-1, 1].
from sklearn.preprocessing import MaxAbsScaler

df = pd.DataFrame(df)

scaler = MaxAbsScaler()
scaled_data = scaler.fit_transform(df)

scaled_df = pd.DataFrame(scaled_data, columns=df.columns)

print(scaled_df)

#Min-Max Scaling is useful when you know the bounds of your data and when the distribution is not Normal.
#Standardization is preferred when the data follows a Gaussian distribution (or normal distribution).
#Robust Scaling is beneficial when the data contains outliers, as it uses the median and IQR which are robust to outliers.
#MaxAbs Scaling is useful when the data is already centered around zero and you want to maintain sparsity in sparse data sets.


# More information about why we should use Feature Scaling


#- The coefficients of linear models are influenced by the scale of the variable.
#- Variables with bigger magnitude dominate over those with smaller magnitude
#- Gradient descent converges much faster on scaled data
#- Feature scaling decrease the time to find support vectors for SVMs
#- Euclidean distances are sensitive to feature magnitude.
#- PCA require the features to be centered at 0.
#- compute data


#The machine learning models affected by the feature scale are:
#- Linear and Logistic Regression
#- Neural Networks
#- Support Vector Machines
#- KNN
#- K-means clustering
#- Principal Component Analysis (PCA)

# Creating Interaction Features
df['incidence_radius_ratio'] = df['pelvic_incidence'] / df['pelvic_radius']
print(df)

#Binning

bins = [0, 25, 50, 100, 150]
labels = ['<25', '25-50', '50-100', '150+']
df['incidence_group'] = pd.cut(df['pelvic_incidence'], bins=bins, labels=labels)
print(df)

#By applying these techniques, you can unlock the full potential of your data and build more accurate machine learning models.
#Remember, there’s no one-size-fits-all approach to feature engineering. Experiment with different techniques, and let your data guide you towards creating features that make your models shine.
# Bunları yaptığımızda modelin doğru tahmin oranını artırırız.
# Dataları round ettiğimizde daha hızlı çalışmasını sağlayabiliriz.